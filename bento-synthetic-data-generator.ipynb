{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields, asdict\n",
    "import yaml\n",
    "from typing import List\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE CLASSES\n",
    "#dataclass for model properties\n",
    "@dataclass\n",
    "class ModelProperty:\n",
    "    name: str = field(default = \"\")\n",
    "    desc: str = field(default = \"\")\n",
    "    value_type: str = field(default = \"\")\n",
    "    value_list: List[str] = field(default_factory=list)\n",
    "    synthetic_value_list: List[str] = field(default_factory=list)\n",
    "    units: List[str] = field(default_factory=list)\n",
    "    pattern: str = field(default = \"\")\n",
    "    url: str = field(default = \"\")\n",
    "    req: bool = field(default = False)\n",
    "    private: bool = field(default = False)\n",
    "    minimum: str = field(default = \"\")\n",
    "    maximum: str = field(default = \"\")\n",
    "    exclusiveMinimum:  str = field(default = \"\")\n",
    "    exclusiveMaximum:  str = field(default = \"\")\n",
    "    \n",
    "    def emit_value(self):\n",
    "        property_data_value = \"\"\n",
    "        if self.synthetic_value_list:\n",
    "            property_data_value = random.choice(self.synthetic_value_list)\n",
    "            return property_data_value\n",
    "        if self.value_list:\n",
    "            property_data_value = random.choice(self.value_list)\n",
    "            return property_data_value\n",
    "        if self.value_type == 'string':\n",
    "            base_list = [\"a_bene_placito\",\n",
    "                         \"barba_crescit_caput_nescit\",\n",
    "                         \"cacatum_non_est_pictum\",\n",
    "                         \"damnant_quod_non_intellegunt\",\"e_causa_ignota\",\n",
    "                         \"faber_est_suae_quisque_fortunae\",\n",
    "                         \"Gallia_est_omnis_divisa_in_partes_tres\",\"haec_olim_meminisse_iuvabit\",\n",
    "                         \"id_quod_plerumque_accidit\",\"imperium_in_imperio\",\"labor_ipse_voluptas\",\n",
    "                         \"Macte_animo_Generose_puer_sic_itur_ad_astra\",\"nanos_gigantum_humeris_insidentes\",\n",
    "                         \"nascentes_morimur_finisque_ab_origine_pendet\",\"O_Tite_tute_Tati_tibi_tanta_tyranne_tulisti\",\n",
    "                         \"Obedientia_civium_urbis_felicitas\",\"pace_tua\",\"saltus_in_demonstrando\",\n",
    "                         \"salus_in_arduis\",\"sapiens_qui_prospicit\",\"scientia_et_labor\",\"scientia_et_sapientia\",\n",
    "                         \"scientia_imperii_decus_et_tutamen\",\"scientia,_aere_perennius\",\"scientiae_cedit_mare\",\n",
    "                         \"scientiae_et_patriae\"]\n",
    "            property_data_value = random.choice(base_list)\n",
    "            return property_data_value\n",
    "        if self.value_type == 'number':\n",
    "            if type(self.minimum) == float and type(self.maximum) == float:\n",
    "                property_data_value = random.uniform(self.minimum, self.maximum)\n",
    "            else:\n",
    "                property_data_value = random.uniform(10.0,1000.0)\n",
    "            property_data_value = round(property_data_value, 2)\n",
    "            return property_data_value\n",
    "        if self.value_type == 'boolean':\n",
    "            property_data_value = random.choice([True, False])\n",
    "            return property_data_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for model nodes\n",
    "@dataclass\n",
    "class ModelNode:\n",
    "    name: str = field(default = \"\")\n",
    "    properties: List[ModelProperty] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for the ends of a model relationship\n",
    "@dataclass\n",
    "class ModelEnds:\n",
    "    source_node: ModelNode\n",
    "    destination_node: ModelNode\n",
    "    multiplicity: str = field(default = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for model relationships\n",
    "@dataclass\n",
    "class ModelEdge:\n",
    "    name: str\n",
    "    ends_list: List[ModelEnds] = field(default_factory=list)\n",
    "    properties_list: List[ModelProperty] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for mock data nodes\n",
    "@dataclass\n",
    "class DataNode:\n",
    "    node_id: str\n",
    "    parent_node_id_list: list\n",
    "    child_node_id_list: list\n",
    "    node_type: str\n",
    "    node_attributes: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for mock data relationships\n",
    "@dataclass\n",
    "class DataEdge:\n",
    "    edge_id: str\n",
    "    edge_type: str\n",
    "    edge_attributes: dict\n",
    "    source_node: DataNode\n",
    "    destination_node: DataNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Graph:\n",
    "    dict_of_data_nodes: defaultdict(list)\n",
    "    dict_of_data_edges: {}\n",
    "    \n",
    "    def print_data(self, node_type = 'all'):\n",
    "        if node_type == 'all':\n",
    "            for node_type_key in data_graph.dict_of_data_nodes:\n",
    "                node_values_dict = defaultdict(list)\n",
    "                df = pd.DataFrame()\n",
    "                for node in data_graph.dict_of_data_nodes[node_type_key]:\n",
    "                    node_values_dict['type'].append(node.node_type)\n",
    "                \n",
    "                if  node.parent_node_id_list:\n",
    "                    node_values_dict['parent_id'].append(node.parent_node_id_list[0])\n",
    "\n",
    "                node_values_dict['node_id'].append(node.node_id)\n",
    "                for node_prop in node.node_attributes:\n",
    "                    node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "                \n",
    "                for node_values_key in node_values_dict:\n",
    "                    df[node_values_key] = node_values_dict[node_values_key]\n",
    "                \n",
    "                file_name = node_type_key + \".csv\"\n",
    "                df.to_csv(file_name)\n",
    "            \n",
    "            return\n",
    "        else:\n",
    "            if node_type in data_graph.dict_of_data_nodes:\n",
    "                node_values_dict = defaultdict(list)\n",
    "                df = pd.DataFrame()\n",
    "                for node in data_graph.dict_of_data_nodes[node_type]:\n",
    "                    node_values_dict['type'].append(node.node_type)\n",
    "                \n",
    "                if  node.parent_node_id_list:\n",
    "                    node_values_dict['parent_id'].append(node.parent_node_id_list[0])\n",
    "                \n",
    "                node_values_dict['node_id'].append(node.node_id)\n",
    "                \n",
    "                for node_prop in node.node_attributes:\n",
    "                    node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "                \n",
    "                for node_values_key in node_values_dict:\n",
    "                    df[node_values_key] = node_values_dict[node_values_key]\n",
    "                \n",
    "                file_name = node_type_key + \".csv\"\n",
    "                df.to_csv(file_name)\n",
    "                return\n",
    "            else:\n",
    "                print(\"node type not found in graph.\")\n",
    "                return\n",
    "    \n",
    "    def fill_graph(self, listOfProps, model_nodes_dict, model_props_dict):\n",
    "        for node_type in self.dict_of_data_nodes:\n",
    "            listOfNodeProps = model_nodes_dict[node_type].properties\n",
    "            listOfDataNodes = self.dict_of_data_nodes[node_type]\n",
    "            for data_node in listOfDataNodes:\n",
    "                for prop in listOfNodeProps:\n",
    "                    if prop.name in listOfProps[data_node.node_type]:\n",
    "                        data_node.node_attributes[prop.name] = model_props_dict[prop.name].emit_value()\n",
    "        return\n",
    "    \n",
    "    def get_dict_of_data_nodes(self):\n",
    "        return self.dict_of_data_nodes\n",
    "    \n",
    "    def get_dict_of_data_edges(self):\n",
    "        return self.dict_of_data_edges\n",
    "    \n",
    "    def get_in_degree(self, input_node_id):\n",
    "        for key in self.dict_of_data_nodes:\n",
    "            for node in dict_of_data_nodes[key]:\n",
    "                if node.node_id == input_node_id:\n",
    "                    return len(node.parent_node_id_list)\n",
    "    \n",
    "    def get_out_degree(self, input_node_id):\n",
    "        for key in self.dict_of_data_nodes:\n",
    "            for node in dict_of_data_nodes[key]:\n",
    "                if node.node_id == input_node_id:\n",
    "                    return len(node.child_node_id_list)\n",
    "    \n",
    "    def summary(self):\n",
    "        summary = {}\n",
    "        summary['Nodes Summary'] = {}\n",
    "        summary['Edges Summary']  = {}\n",
    "        \n",
    "        for node_type in self.dict_of_data_nodes:\n",
    "            node_count = len(dict_of_data_nodes[node_type])\n",
    "            summary['Nodes Summary'].update({node_type: node_count})\n",
    "        \n",
    "        edge_type_list = []\n",
    "        for edge in dict_of_data_edges.values():\n",
    "            edge_type_list.append(edge.edge_type)\n",
    "        edge_type_counter = Counter(edge_type_list)\n",
    "        for edge_type, edge_type_count in edge_type_counter.items():\n",
    "            summary['Edges Summary'].update({edge_type: edge_type_count})\n",
    "        \n",
    "        return summary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BEGIN READ SECTION######\n",
    "dict_of_model_properties = {}\n",
    "dict_of_model_nodes = {}\n",
    "model_graph = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_files = 'configuration_files_bento.yaml'\n",
    "with open(configuration_files) as f:\n",
    "    configuration_files = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ MODEL FILES AND FILE WITH SYNTHETIC VALUES\n",
    "#FOR BENTO\n",
    "NODE_FILE = configuration_files['NODE_FILE']\n",
    "PROP_FILE = configuration_files['PROP_FILE']\n",
    "SYNTHETIC_DATA_FILE = configuration_files['SYNTHETIC_DATA_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_values_df = pd.read_excel(io = SYNTHETIC_DATA_FILE,\n",
    "                        sheet_name = \"Sheet1\",\n",
    "                        engine = \"openpyxl\",\n",
    "                        keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROP_FILE) as f:\n",
    "    property_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for property_name in property_data['PropDefinitions'].keys():\n",
    "        property_value_type = property_data['PropDefinitions'][property_name]['Type']\n",
    "        name = property_name\n",
    "        desc = \"\"\n",
    "        req= \"\"\n",
    "        value_type = \"\"\n",
    "        value_list = []\n",
    "        synthetic_value_list = []\n",
    "        units = []\n",
    "        private = \"\"\n",
    "        pattern = \"\"\n",
    "        url = \"\"\n",
    "        minimum = \"\"\n",
    "        maximum = \"\"\n",
    "        exclusiveMinimum = \"\"\n",
    "        exclusiveMaximum = \"\"\n",
    "\n",
    "        if type(property_value_type) is str:\n",
    "            name = property_name\n",
    "            value_type = property_value_type\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            if 'minimum' in property_data['PropDefinitions'][property_name]:\n",
    "                minimum = property_data['PropDefinitions'][property_name]['minimum']\n",
    "            if 'maximum' in property_data['PropDefinitions'][property_name]:\n",
    "                maximum = property_data['PropDefinitions'][property_name]['maximum']\n",
    "            if property_name in synthetic_values_df.columns:\n",
    "                synthetic_value_list = [x for x in synthetic_values_df[property_name].tolist() if x != '']\n",
    "                \n",
    "        if type(property_value_type) is list:\n",
    "            value_type = \"list\"\n",
    "            value_list = property_value_type\n",
    "            #add section on reading the url to create a value list if property_value_type contains a url.\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            if property_name in synthetic_values_df.columns:\n",
    "                synthetic_value_list = [x for x in synthetic_values_df[property_name].tolist() if x != '']\n",
    "                \n",
    "        if type(property_value_type) is dict:\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'value_type' in property_value_type:\n",
    "                value_type = property_value_type['value_type']\n",
    "            if 'units' in property_value_type:\n",
    "                units = property_value_type['units']\n",
    "            if 'pattern' in property_value_type:\n",
    "                pattern = property_value_type['pattern']\n",
    "                value_type = \"regex\"\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            if 'minimum' in property_data['PropDefinitions'][property_name]:\n",
    "                minimum = property_data['PropDefinitions'][property_name]['minimum']\n",
    "            if 'maximum' in property_data['PropDefinitions'][property_name]:\n",
    "                maximum = property_data['PropDefinitions'][property_name]['maximum']\n",
    "            if property_name in synthetic_values_df.columns:\n",
    "                synthetic_value_list = [x for x in synthetic_values_df[property_name].tolist() if x != '']\n",
    "            \n",
    "        dict_of_model_properties[property_name] = ModelProperty(name = name, desc = desc, \n",
    "                                                                    value_type = value_type, value_list = value_list,\n",
    "                                                                    units = units, url = url, req = req, private = private, minimum = minimum, maximum = maximum,\n",
    "                                                                    exclusiveMinimum = exclusiveMinimum, exclusiveMaximum = exclusiveMaximum, synthetic_value_list = synthetic_value_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODE_FILE) as f:\n",
    "    node_data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_data['Nodes']\n",
    "\n",
    "for node_name in nodes.keys():\n",
    "    #print(node_name, nodes[node_name]['Props'])\n",
    "    if nodes[node_name]['Props']:\n",
    "        property_list = [dict_of_model_properties[property_name] for property_name in nodes[node_name]['Props']]\n",
    "    else:\n",
    "        property_list = []\n",
    "    dict_of_model_nodes[node_name] = ModelNode(name = node_name, properties = property_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = node_data['Relationships']\n",
    "\n",
    "for edge_name in edges.keys():\n",
    "    Ends_list = []\n",
    "    Property_list = []\n",
    "    edge_multiplicity = edges[edge_name]['Mul']\n",
    "    \n",
    "    for edge_pair in edges[edge_name]['Ends']:\n",
    "        source_node = edge_pair['Src']\n",
    "        destination_node = edge_pair['Dst']\n",
    "        if 'Mul' in edge_pair:\n",
    "            edge_multiplicity = edge_pair['Mul']\n",
    "        Ends_list.append(ModelEnds(source_node = dict_of_model_nodes[source_node], destination_node = dict_of_model_nodes[destination_node], multiplicity = edge_multiplicity))\n",
    "        \n",
    "    \n",
    "    if 'Props' in edges[edge_name] and edges[edge_name]['Props']:\n",
    "        property_list = [dict_of_model_properties[property_name] for property_name in edges[edge_name]['Props']]\n",
    "    else:\n",
    "        property_list = []\n",
    "    \n",
    "    model_graph[edge_name] = ModelEdge(name = edge_name, ends_list = Ends_list, properties_list = Property_list)\n",
    "#END READ MODEL FILES\n",
    "######END READ SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BEGIN SPAWN SECTION######\n",
    "dict_of_data_nodes = defaultdict(list)\n",
    "dict_of_data_edges = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ DATA SPECS FILE\n",
    "#FOR BENTO\n",
    "DATA_SPEC_FILE = configuration_files['DATA_SPEC_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_SPEC_FILE) as f:\n",
    "    data_spec = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create head data node object\n",
    "head_node_type = data_spec['HeadNode']['name']\n",
    "head_node_count = data_spec['HeadNode']['count']\n",
    "id_prefix = data_spec['HeadNode']['Prefix']\n",
    "dst_node_type = head_node_type\n",
    "# random a set of id without duplicate\n",
    "node_id_number_list = random.sample(range(10**5, 10**6), head_node_count + 1)\n",
    "head_node_index = 0\n",
    "for count in range(head_node_count):\n",
    "    # node_id = id_prefix + \"_\" + str(random.randint(10**5, 10**6))\n",
    "    node_id = id_prefix + \"-\" + str(node_id_number_list[head_node_index])# for bento\n",
    "    head_node_index += 1\n",
    "    parent_node_id_list = []\n",
    "    child_node_id_list = []\n",
    "    node_type = head_node_type\n",
    "    node_attributes = {}\n",
    "    data_node = DataNode(node_id = node_id, parent_node_id_list = parent_node_id_list, child_node_id_list = child_node_id_list,\n",
    "                         node_type = node_type, node_attributes = {})\n",
    "    dict_of_data_nodes[head_node_type].append(data_node)\n",
    "\n",
    "edge_specs = data_spec['RelationshipSpecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dst_node_type in edge_specs.keys():\n",
    "    dst_data_nodes_list = dict_of_data_nodes[dst_node_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "includeNodes = data_spec['IncludeNodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEdgeType(node_data, src_node_type, dst_node_type):\n",
    "    for edge_type in node_data['Relationships']:\n",
    "        if node_data['Relationships'][edge_type]['Ends'][0]['Src'] == src_node_type and node_data['Relationships'][edge_type]['Ends'][0]['Dst'] == dst_node_type:\n",
    "            return edge_type\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a skeleton data graph.\n",
    "#Create a skeleton data graph.\n",
    "def SpawnNodes():\n",
    "    created_children = []\n",
    "    children = []\n",
    "    for dst_node_type in edge_specs.keys():\n",
    "        if dst_node_type in dict_of_data_nodes and dst_node_type not in created_children:\n",
    "            dst_data_nodes_list = dict_of_data_nodes[dst_node_type]\n",
    "            # for dst_data_node in dst_data_nodes_list:\n",
    "            for src_node_type in edge_specs[dst_node_type].keys():\n",
    "                # print(dst_node_type, src_node_type)\n",
    "                node_counter = includeNodes[src_node_type]['NodeCount']\n",
    "                node_distribution = edge_specs[dst_node_type][src_node_type]['SrcNodeCount']\n",
    "                id_prefix = includeNodes[src_node_type]['Prefix']\n",
    "                # random a set of id without duplicate\n",
    "                node_id_number_list = random.sample(range(10**5, 10**6), node_counter)\n",
    "                node_index = 0\n",
    "                parent_node_index = 0\n",
    "                parent_node_length = len(dst_data_nodes_list)\n",
    "                step = int(node_counter / parent_node_length)\n",
    "                # if the distribution is random\n",
    "                # print(node_counter, parent_node_length)\n",
    "                if node_distribution == 'random':\n",
    "                    node_counter_list = range(node_counter)\n",
    "                    random_split_points = random.sample(node_counter_list, parent_node_length - 1)\n",
    "                    random_split_points.sort()\n",
    "                    random_split_points_index = 0\n",
    "                for count in range(node_counter):\n",
    "                    if node_distribution == 'fixed':\n",
    "                        if node_index % step == 0 and node_index != 0:\n",
    "                            parent_node_index += 1\n",
    "                    elif node_distribution == 'random':\n",
    "                        #print(node_index)\n",
    "                        if random_split_points_index < len(random_split_points):\n",
    "                            if node_index == random_split_points[random_split_points_index]:\n",
    "                                random_split_points_index += 1\n",
    "                                parent_node_index += 1\n",
    "                    if parent_node_index > parent_node_length - 1:\n",
    "                            parent_node_index = parent_node_length - 1\n",
    "                    # node_id = id_prefix + \"_\" + str(random.randint(10**5, 10**6))\n",
    "                    node_id = id_prefix + \"-\" + str(node_id_number_list[node_index])\n",
    "                    if src_node_type not in children:\n",
    "                        node_index += 1\n",
    "                        parent_node_id_list = []\n",
    "                        parent_node_id_list.append(dst_data_nodes_list[parent_node_index].node_id)\n",
    "                        child_node_id_list = []\n",
    "                        node_type = src_node_type\n",
    "                        node_attributes = {}\n",
    "                        src_data_node = DataNode(node_id = node_id, parent_node_id_list = parent_node_id_list, child_node_id_list = child_node_id_list,\n",
    "                                             node_type = node_type, node_attributes = {}) #source node created.\n",
    "                        dict_of_data_nodes[src_node_type].append(src_data_node) #source node added to the dict of nodes.\n",
    "\n",
    "                        dst_data_nodes_list[parent_node_index].child_node_id_list.append(node_id) #add created source node to the child nodes list for dst node.\n",
    "                    elif src_node_type in children:\n",
    "                        dict_of_data_nodes[src_node_type][node_index].parent_node_id_list.append(dst_data_nodes_list[parent_node_index].node_id)\n",
    "                        node_index += 1\n",
    "                    edge_id = \"edge\" + \"_\" + str(random.randint(10**5, 10**6))\n",
    "                    edge_type = findEdgeType(node_data, src_node_type, dst_node_type)\n",
    "                    # edge_type = edge_specs[dst_node_type][src_node_type]['EdgeType']\n",
    "                    edge_attributes = {}\n",
    "                    data_edge = DataEdge(edge_id = edge_id, edge_type = edge_type, source_node = src_data_node, \n",
    "                                     destination_node = dst_data_nodes_list[parent_node_index], edge_attributes = edge_attributes) #edge created.\n",
    "                    dict_of_data_edges[edge_id] = data_edge #edge added to the dict of edges.\n",
    "                children.append(src_node_type)\n",
    "            created_children.append(dst_node_type)\n",
    "    data_graph = Graph(dict_of_data_nodes = dict_of_data_nodes, dict_of_data_edges = dict_of_data_edges)\n",
    "    return data_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create skeleton data graph\n",
    "data_graph = SpawnNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nodes Summary': {'institution': 1, 'program': 1, 'laboratory_procedure': 2, 'study': 2, 'study_subject': 40, 'sample': 120, 'diagnosis': 80, 'file': 1200, 'follow_up': 80, 'demographic_data': 40, 'stratification_factor': 40, 'therapeutic_procedure': 160}, 'Edges Summary': {'program_of_institution': 1, 'study_of_program': 2, 'laboratory_procedure_of_program': 2, 'file_of_laboratory_procedure': 1198, 'file_of_sample': 1200, 'sample_processed_by': 120, 'study_subject_of_study': 40, 'fu_of_study_subject': 80, 'diagnosis_of_study_subject': 79, 'demographic_of_study_subject': 39, 'sample_of_study_subject': 120, 'sf_of_study_subject': 40, 'tp_of_diagnosis': 160}}\n"
     ]
    }
   ],
   "source": [
    "#Examine skeleton data graph\n",
    "# data_graph.summary()\n",
    "print(data_graph.summary())\n",
    "######END SPAWN SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_FILE = configuration_files['ID_FILE']\n",
    "with open(ID_FILE) as f:\n",
    "    id_field_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "relationship_node_dict = {}\n",
    "node_id_field_dict = {}\n",
    "node_list = []\n",
    "for parent_id in data_spec['RelationshipSpecs']:\n",
    "    for node_id in data_spec['RelationshipSpecs'][parent_id]:\n",
    "        relationship_node = {}\n",
    "        relationship_node['parent_id'] = []\n",
    "        relationship_node['parent_id_field'] = []\n",
    "        relationship_node['node_id'] = node_id\n",
    "        relationship_node['parent_id'].append(parent_id)\n",
    "        if node_id in id_field_data['Properties']['id_fields']:\n",
    "            relationship_node['node_id_field'] = id_field_data['Properties']['id_fields'][node_id]\n",
    "        else:\n",
    "            relationship_node['node_id_field'] ='node_id'\n",
    "        if parent_id in id_field_data['Properties']['id_fields']:\n",
    "            relationship_node['parent_id_field'].append(id_field_data['Properties']['id_fields'][parent_id])\n",
    "        else:\n",
    "            relationship_node['parent_id_field'].append('parent_id')\n",
    "        if node_id not in relationship_node_dict.keys():\n",
    "            relationship_node_dict[node_id] = relationship_node\n",
    "        else:\n",
    "            relationship_node_dict[node_id]['parent_id'].append(relationship_node['parent_id'][0])\n",
    "            relationship_node_dict[node_id]['parent_id_field'].append(relationship_node['parent_id_field'][0])\n",
    "\n",
    "for node_type in data_graph.dict_of_data_nodes:\n",
    "    if node_type in id_field_data['Properties']['id_fields']:\n",
    "        node_id_field_dict[node_type] = id_field_data['Properties']['id_fields'][node_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetParentIDField(node_type, index):\n",
    "    if node_type in relationship_node_dict:\n",
    "        return relationship_node_dict[node_type]['parent_id'][index]+'.'+relationship_node_dict[node_type]['parent_id_field'][index]\n",
    "    else:\n",
    "        return 'parent_id'\n",
    "def GetNodeIDField(node_type):\n",
    "    if node_type in node_id_field_dict:\n",
    "        return node_id_field_dict[node_type]\n",
    "    else:\n",
    "        return 'node_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BEGIN FILL SECTION######\n",
    "includePropsList = data_spec['IncludeProperties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_graph.fill_graph(listOfProps = includePropsList, \n",
    "                      model_nodes_dict = dict_of_model_nodes, \n",
    "                      model_props_dict = dict_of_model_properties)\n",
    "######END FILL SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######PRINT DATA FILES######\n",
    "child_node_id_dict = {}\n",
    "child_node_id_list = []\n",
    "for node_type in data_graph.dict_of_data_nodes:\n",
    "    node_values_dict = defaultdict(list)\n",
    "    df = pd.DataFrame()\n",
    "    position = 0\n",
    "    for node in data_graph.dict_of_data_nodes[node_type]:\n",
    "        node_values_dict['type'].append(node.node_type)\n",
    "        if node.parent_node_id_list:\n",
    "            index = 0\n",
    "            for parent_node_id in node.parent_node_id_list:\n",
    "                if node.node_id not in child_node_id_list:\n",
    "                    node_values_dict[GetParentIDField(node_type, index)].append(parent_node_id) #parent\n",
    "                else:\n",
    "                    node_values_dict[GetParentIDField(node_type, index)].append(child_node_id_dict[node.node_id])\n",
    "                    data_graph.dict_of_data_nodes[node_type][position].parent_node_id_list[0] = child_node_id_dict[node.node_id]\n",
    "                index += 1\n",
    "        if GetNodeIDField(node_type) not in node.node_attributes:\n",
    "            node_values_dict[GetNodeIDField(node_type)].append(node.node_id) #node\n",
    "        if GetNodeIDField(node_type) in node.node_attributes and GetNodeIDField(node_type) in synthetic_values_df.keys():\n",
    "            res = synthetic_values_df[GetNodeIDField(node_type)].tolist()\n",
    "            # res = synthetic_values_df[GetNodeIDField(node_type)].tolist()\n",
    "            trim_res = [i for i in res if i]\n",
    "            # if the node_type has more nodes than the values of all usable node_id\n",
    "            if len(data_graph.dict_of_data_nodes[node_type]) > len(trim_res):\n",
    "                error_message = 'node ' + node_type + ' is running out of all usable node_ids from ' + GetNodeIDField(node_type) + '.'\n",
    "                # delete all previous generate tsv files\n",
    "                mydir = os.getcwd() + configuration_files['OUTPUT_FOLDER']\n",
    "                filelist = [ f for f in os.listdir(mydir) if f.endswith(\".tsv\") ]\n",
    "                for f in filelist:\n",
    "                    os.remove(os.path.join(mydir, f))\n",
    "                sys.exit(error_message)\n",
    "            new_node_id_list = []\n",
    "            for new_node in data_graph.dict_of_data_nodes[node_type]:\n",
    "                new_node_id_list.append(new_node.node_attributes[GetNodeIDField(node_type)])\n",
    "            new_node_id_list_counter = Counter(new_node_id_list)\n",
    "            reselect_value = False\n",
    "            #Check if the new node_id list has duplicate node_id\n",
    "            for value in new_node_id_list_counter.values():\n",
    "                if value > 1:\n",
    "                    reselect_value = True\n",
    "            if reselect_value == True:\n",
    "                new_value_list = random.sample(trim_res, len(new_node_id_list))\n",
    "                for i in range(len(data_graph.dict_of_data_nodes[node_type])):\n",
    "                    data_graph.dict_of_data_nodes[node_type][i].node_attributes[GetNodeIDField(node_type)] = new_value_list[i]\n",
    "            data_graph.dict_of_data_nodes[node_type][position].node_id = data_graph.dict_of_data_nodes[node_type][position].node_attributes[GetNodeIDField(node_type)]\n",
    "            for child_node_id in data_graph.dict_of_data_nodes[node_type][position].child_node_id_list:\n",
    "                child_node_id_list.append(child_node_id)\n",
    "                child_node_id_dict[child_node_id] = data_graph.dict_of_data_nodes[node_type][position].node_id\n",
    "        if GetNodeIDField(node_type) in node.node_attributes and GetNodeIDField(node_type) not in synthetic_values_df.keys():\n",
    "            # if the user adds the id field into the data spec document accidentally\n",
    "            del node.node_attributes[GetNodeIDField(node_type)]\n",
    "            node_values_dict[GetNodeIDField(node_type)].append(node.node_id) #node\n",
    "        for node_prop in node.node_attributes:\n",
    "            # print(node_type)\n",
    "            # print(node.node_attributes)\n",
    "            node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "        position+=1\n",
    "    for node_values_key in node_values_dict:\n",
    "        df[node_values_key] = node_values_dict[node_values_key]\n",
    "    \n",
    "    file_name = configuration_files['OUTPUT_FOLDER'] + node_type + \".tsv\"\n",
    "    if not os.path.exists(configuration_files['OUTPUT_FOLDER']):\n",
    "        os.mkdir(configuration_files['OUTPUT_FOLDER'])\n",
    "    df.to_csv(file_name, sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader\n",
    "from icdc_schema import ICDC_Schema\n",
    "from neo4j import GraphDatabase\n",
    "from props import Props\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<14>1 2022-03-17T22:12:23.132Z - - 21332 - - INFO: (ICDC Schema) Reading schema file: ./bento_configuration_files/bento_tailorx_model_file.yaml ...\n",
      "<14>1 2022-03-17T22:12:23.172Z - - 21332 - - INFO: (ICDC Schema) Reading schema file: ./bento_configuration_files/bento_tailorx_model_properties.yaml ...\n",
      "<14>1 2022-03-17T22:12:23.751Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/demographic_data.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.758Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/diagnosis.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.769Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/file.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.844Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/follow_up.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.849Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/institution.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.857Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/laboratory_procedure.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.860Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/program.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.865Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/sample.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.875Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/stratification_factor.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.881Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/study.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.885Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/study_subject.tsv\" ...\n",
      "<14>1 2022-03-17T22:12:23.893Z - - 21332 - - INFO: (Data Loader) Validating file \"./bento_data_files/therapeutic_procedure.tsv\" ...\n"
     ]
    }
   ],
   "source": [
    "file_list = [f for f in os.listdir(configuration_files['OUTPUT_FOLDER']) if os.path.isfile(os.path.join(configuration_files['OUTPUT_FOLDER'], f))]\n",
    "for i in range(0, len(file_list)):\n",
    "    file_list[i] = configuration_files['OUTPUT_FOLDER'] + file_list[i]\n",
    "uri = 'bolt://localhost:7687'\n",
    "user = 'neo4j'\n",
    "password = 'neo4j'\n",
    "driver = GraphDatabase.driver(uri, auth = (user, password))\n",
    "props = Props(configuration_files['ID_FILE'])\n",
    "schema = ICDC_Schema([configuration_files['NODE_FILE'], configuration_files['PROP_FILE']], props)\n",
    "loader = DataLoader(driver, schema)\n",
    "fileValidationResult = loader.validate_files(False, file_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationshipValidation(dict_of_data_edges, node_data, includeNodes):\n",
    "    for edge in dict_of_data_edges.values():\n",
    "        mul = node_data['Relationships'][edge.edge_type]['Mul']\n",
    "        prefix = includeNodes[edge.source_node.node_type]['Prefix']\n",
    "        child_node_id_list = []\n",
    "        for child_node_id in edge.destination_node.child_node_id_list:\n",
    "            if prefix in child_node_id:\n",
    "                child_node_id_list.append(child_node_id)\n",
    "        if mul == 'one_to_one' and len(child_node_id_list) > 1:\n",
    "            logging.error(edge.source_node.node_type + ' ' + 'one_to_one relationship failed, parent already has a child!')\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation success\n"
     ]
    }
   ],
   "source": [
    "relationshipValidationResult = relationshipValidation(dict_of_data_edges, node_data, includeNodes)\n",
    "if not relationshipValidationResult or not fileValidationResult:\n",
    "    print('Validation fail, delete all files inside the data folder.')\n",
    "    mydir = os.getcwd() + configuration_files['OUTPUT_FOLDER']\n",
    "    filelist = [ f for f in os.listdir(mydir) if f.endswith(\".tsv\") ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))\n",
    "else:\n",
    "    print('Validation success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
