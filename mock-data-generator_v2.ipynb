{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields, asdict\n",
    "import yaml\n",
    "from typing import List\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import pprint\n",
    "import os\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE CLASSES\n",
    "#dataclass for model properties\n",
    "@dataclass\n",
    "class ModelProperty:\n",
    "    name: str = field(default = \"\")\n",
    "    desc: str = field(default = \"\")\n",
    "    value_type: str = field(default = \"\")\n",
    "    value_list: List[str] = field(default_factory=list)\n",
    "    synthetic_value_list: List[str] = field(default_factory=list)\n",
    "    units: List[str] = field(default_factory=list)\n",
    "    pattern: str = field(default = \"\")\n",
    "    url: str = field(default = \"\")\n",
    "    req: bool = field(default = False)\n",
    "    private: bool = field(default = False)\n",
    "    minimum: int = field(default = 0)\n",
    "    maximum: int = field(default = 1000)\n",
    "    exclusiveMinimum:  str = field(default = \"\")\n",
    "    exclusiveMaximum:  str = field(default = \"\")\n",
    "    #dist: str = field(default = 'Uniform')\n",
    "    weights: List[int]= field(default_factory=list)\n",
    "    \n",
    "    def get_weighted_values(self):\n",
    "        self.weights = []\n",
    "        \n",
    "        if Node_Props[\"Minage\"] != '' and self.name  in ['age_at_enrollment', 'age_at_first_cancer_diagnosis']: \n",
    "            self.minimum = eval(Node_Props[\"Minage\"]) \n",
    "            #print(f\"{self.name}, : {self.minimum}\")\n",
    "        if Node_Props[\"Maxage\"] != '' and self.name  in ['age_at_enrollment', 'age_at_first_cancer_diagnosis']:\n",
    "            self.maximum = eval(Node_Props[\"Maxage\"])\n",
    "        \n",
    "        if self.name in weight_dict:\n",
    "            curr_dict = weight_dict[self.name]   \n",
    "            if True in curr_dict: curr_dict.update({\"Yes\": curr_dict.pop(True)})\n",
    "            if False in curr_dict: curr_dict.update({\"No\": curr_dict.pop(False)})\n",
    "            \n",
    "            if (len(self.value_list) - len(curr_dict)) > 0:\n",
    "                missing_pct = round((1-sum(curr_dict.values())) / (len(self.value_list) - len(curr_dict)) , 3)\n",
    "            else:\n",
    "                missing_pct = 0\n",
    "            weight_list = []\n",
    "            for curr_value in self.value_list:\n",
    "                if curr_value in curr_dict:\n",
    "                    self.weights.append(curr_dict[curr_value])\n",
    "                else:\n",
    "                    self.weights.append(missing_pct)\n",
    "        if sum(self.weights) == 0:\n",
    "            self.weights = []\n",
    "\n",
    "    \n",
    "    def emit_value(self):\n",
    "        #relationship_node_dict[node_type]['parent_id_field'][index][0]\n",
    "        if self.name  in ['participant_race'] and \"multi_race_opt\" in weight_dict:\n",
    "            uni_choice = random.choices([1,2], weights=weight_dict[\"multi_race_opt\"].values())[0]\n",
    "        else:\n",
    "            uni_choice = 1\n",
    "        \n",
    "        curr_choice = 1\n",
    "        \n",
    "        property_data_value = \"\"\n",
    "        output_value = []\n",
    "        while curr_choice <= uni_choice:\n",
    "            self.get_weighted_values()\n",
    "            if self.name  in ['cancer_diagnosis_primary_site'] and cancer_location_type  == 'ICD-O-3':  #use codes\n",
    "                list_of_lists = cancer_location_df[\"ICD-0-3 Code\"].values.tolist()\n",
    "                self.value_list = [item for sublist in list_of_lists for item in sublist]\n",
    "            if self.name  in ['cancer_diagnosis_disease_morphology'] and cancer_histology_type  == 'ICD-O-3':  #use codes\n",
    "                self.value_list = cancer_histology_df[\"ICD-0-3 Code\"].values.tolist()\n",
    "                #self.value_list = [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "            \n",
    "            #print(f\"{self.name} has weights {self.value_list}\")\n",
    "            #if  self.name  in ['cancer_diagnosis_primary_site']:  #use translantion file instead\n",
    "            #    self.value_list = cancer_location_df[\"ICD-0-3 Code\"].values.flatten().tolist()\n",
    "            \n",
    "            if self.synthetic_value_list:\n",
    "                if len(self.weights) == 0:  #no weights provided\n",
    "                    property_data_value = random.choice(self.synthetic_value_list)\n",
    "                else:\n",
    "                    property_data_value = random.choices(self.synthetic_value_list, weights=self.weights)[0]\n",
    "                #return property_data_value\n",
    "            if self.value_list:\n",
    "                if len(self.weights) == 0:  #no weights provided\n",
    "                    property_data_value =  random.choice(self.value_list)\n",
    "                else:\n",
    "                    property_data_value = random.choices(self.value_list, weights=self.weights)[0]\n",
    "                #return property_data_value\n",
    "            if self.value_type == 'string':\n",
    "                base_list = [\"a_bene_placito\",\n",
    "                             \"barba_crescit_caput_nescit\",\n",
    "                             \"cacatum_non_est_pictum\",\n",
    "                             \"damnant_quod_non_intellegunt\",\"e_causa_ignota\",\n",
    "                             \"faber_est_suae_quisque_fortunae\",\n",
    "                             \"Gallia_est_omnis_divisa_in_partes_tres\",\"haec_olim_meminisse_iuvabit\",\n",
    "                             \"id_quod_plerumque_accidit\",\"imperium_in_imperio\",\"labor_ipse_voluptas\",\n",
    "                             \"Macte_animo_Generose_puer_sic_itur_ad_astra\",\"nanos_gigantum_humeris_insidentes\",\n",
    "                             \"nascentes_morimur_finisque_ab_origine_pendet\",\"O_Tite_tute_Tati_tibi_tanta_tyranne_tulisti\",\n",
    "                             \"Obedientia_civium_urbis_felicitas\",\"pace_tua\",\"saltus_in_demonstrando\",\n",
    "                             \"salus_in_arduis\",\"sapiens_qui_prospicit\",\"scientia_et_labor\",\"scientia_et_sapientia\",\n",
    "                             \"scientia_imperii_decus_et_tutamen\",\"scientia,_aere_perennius\",\"scientiae_cedit_mare\",\n",
    "                             \"scientiae_et_patriae\"]\n",
    "                property_data_value = random.choice(base_list)\n",
    "                #return property_data_value\n",
    "            if self.value_type == 'number':\n",
    "                if type(self.minimum) == float and type(self.maximum) == float:\n",
    "                    property_data_value = random.uniform(self.minimum, self.maximum)\n",
    "                elif type(self.minimum) == int and type(self.maximum) == int:\n",
    "                    property_data_value = random.uniform(self.minimum, self.maximum)\n",
    "                else:\n",
    "                    property_data_value = random.uniform(10.0,1000.0)\n",
    "                if \"age\" in self.name:\n",
    "                    property_data_value = round(property_data_value, 0)  #age is a whole number in days\n",
    "                else:\n",
    "                    property_data_value = round(property_data_value, 2)\n",
    "                #return property_data_value\n",
    "            if self.value_type == 'boolean':\n",
    "                property_data_value = random.choice([True, False])\n",
    "                #return property_data_value\n",
    "            if self.value_type == 'integer':\n",
    "                if type(self.minimum) == int and type(self.maximum) == int:\n",
    "                    property_data_value = random.randint(self.minimum, self.maximum)\n",
    "                else:\n",
    "                    property_data_value = random.randint(10,1000)\n",
    "                property_data_value = round(property_data_value, 0)\n",
    "            curr_choice = curr_choice + 1\n",
    "            output_value = output_value + [property_data_value]\n",
    "        \n",
    "        output_value = list(set(output_value)) #get unique records\n",
    "        if len(output_value) > 1:\n",
    "            remove_list = [\"Not Reported\", \"Unknown\", \"Not allowed to collect\"]\n",
    "            for not_allowed in remove_list:\n",
    "                if not_allowed in output_value:\n",
    "                    output_value.remove(not_allowed)\n",
    "                    if len(output_value) == 1:\n",
    "                        break\n",
    "            output_value.sort()  #sort values\n",
    "            output_value = \" | \".join(output_value)  #convert to string\n",
    "        else:\n",
    "            output_value = output_value[0]\n",
    "           \n",
    "        return output_value  #property_data_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for model nodes\n",
    "@dataclass\n",
    "class ModelNode:\n",
    "    name: str = field(default = \"\")\n",
    "    properties: List[ModelProperty] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for the ends of a model relationship\n",
    "@dataclass\n",
    "class ModelEnds:\n",
    "    source_node: ModelNode\n",
    "    destination_node: ModelNode\n",
    "    multiplicity: str = field(default = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for model relationships\n",
    "@dataclass\n",
    "class ModelEdge:\n",
    "    name: str\n",
    "    ends_list: List[ModelEnds] = field(default_factory=list)\n",
    "    properties_list: List[ModelProperty] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for mock data nodes\n",
    "@dataclass\n",
    "class DataNode:\n",
    "    node_id: str\n",
    "    parent_node_id_list: list\n",
    "    child_node_id_list: list\n",
    "    node_type: str\n",
    "    node_attributes: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataclass for mock data relationships\n",
    "@dataclass\n",
    "class DataEdge:\n",
    "    edge_id: str\n",
    "    edge_type: str\n",
    "    edge_attributes: dict\n",
    "    source_node: DataNode\n",
    "    destination_node: DataNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Graph():\n",
    "    dict_of_data_nodes: defaultdict(list) # type: ignore \n",
    "    dict_of_data_edges: {} # type: ignore \n",
    "    \n",
    "    def print_data(self, node_type = 'all'):\n",
    "        if node_type == 'all':\n",
    "            #for node_type_key in data_graph.dict_of_data_nodes:\n",
    "            for node_type_key in self.dict_of_data_nodes:\n",
    "            \n",
    "                node_values_dict = defaultdict(list)\n",
    "                df = pd.DataFrame()\n",
    "                #for node in data_graph.dict_of_data_nodes[node_type_key]:\n",
    "                for node in self.dict_of_data_nodes[node_type_key]:\n",
    "                    node_values_dict['type'].append(node.node_type)\n",
    "                \n",
    "                    if  node.parent_node_id_list:\n",
    "                        node_values_dict['parent_id'].append(node.parent_node_id_list[0])\n",
    "\n",
    "                    node_values_dict['node_id'].append(node.node_id)\n",
    "                    for node_prop in node.node_attributes:\n",
    "                        node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "                    \n",
    "                    for node_values_key in node_values_dict:\n",
    "                        df[node_values_key] = node_values_dict[node_values_key]\n",
    "                \n",
    "                    file_name = node_type_key + \".csv\"\n",
    "                    df.to_csv(file_name)\n",
    "            return\n",
    "        else:\n",
    "            print(self.dict_of_data_nodes)\n",
    "            if node_type in self.dict_of_data_nodes:\n",
    "                node_values_dict = defaultdict(list)\n",
    "                df = pd.DataFrame()\n",
    "                for node in self.dict_of_data_nodes[node_type]:\n",
    "                    node_values_dict['type'].append(node.node_type)\n",
    "                \n",
    "                    if  node.parent_node_id_list:\n",
    "                        node_values_dict['parent_id'].append(node.parent_node_id_list[0])\n",
    "                    \n",
    "                    node_values_dict['node_id'].append(node.node_id)\n",
    "                    \n",
    "                    for node_prop in node.node_attributes:\n",
    "                        node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "                    \n",
    "                    print(node_values_dict)\n",
    "                    for node_values_key in node_values_dict:\n",
    "                        df[node_values_key] = node_values_dict[node_values_key]\n",
    "                    \n",
    "                    file_name = node_type_key + \".csv\"\n",
    "                    df.to_csv(file_name)\n",
    "                return\n",
    "            else:\n",
    "                print(\"node type not found in graph.\")\n",
    "                return\n",
    "    \n",
    "    def fill_graph(self, listOfProps, model_nodes_dict, model_props_dict): # , cancer_dict, tumor_dict):\n",
    "        for node_type in self.dict_of_data_nodes:\n",
    "            listOfNodeProps = model_nodes_dict[node_type].properties            \n",
    "            listOfDataNodes = self.dict_of_data_nodes[node_type]\n",
    "            try:\n",
    "                print(f\"Populating node_type for {node_type}\")                \n",
    "                for data_node in listOfDataNodes:\n",
    "                    for prop in listOfNodeProps:\n",
    "                        if prop.name in listOfProps[data_node.node_type]:\n",
    "                            if model_props_dict[prop.name].emit_value() != None:\n",
    "                                data_node.node_attributes[prop.name] = model_props_dict[prop.name].emit_value()\n",
    "                                # print(data_node.node_attributes[prop.name])\n",
    "                            else:\n",
    "                                # if the value type can no be identified, then change type to string\n",
    "                                base_string_list = [\"a_bene_placito\",\n",
    "                                \"barba_crescit_caput_nescit\",\n",
    "                                \"cacatum_non_est_pictum\",\n",
    "                                \"damnant_quod_non_intellegunt\",\"e_causa_ignota\",\n",
    "                                \"faber_est_suae_quisque_fortunae\",\n",
    "                                \"Gallia_est_omnis_divisa_in_partes_tres\",\"haec_olim_meminisse_iuvabit\",\n",
    "                                \"id_quod_plerumque_accidit\",\"imperium_in_imperio\",\"labor_ipse_voluptas\",\n",
    "                                \"Macte_animo_Generose_puer_sic_itur_ad_astra\",\"nanos_gigantum_humeris_insidentes\",\n",
    "                                \"nascentes_morimur_finisque_ab_origine_pendet\",\"O_Tite_tute_Tati_tibi_tanta_tyranne_tulisti\",\n",
    "                                \"Obedientia_civium_urbis_felicitas\",\"pace_tua\",\"saltus_in_demonstrando\",\n",
    "                                \"salus_in_arduis\",\"sapiens_qui_prospicit\",\"scientia_et_labor\",\"scientia_et_sapientia\",\n",
    "                                \"scientia_imperii_decus_et_tutamen\",\"scientia,_aere_perennius\",\"scientiae_cedit_mare\",\n",
    "                                \"scientiae_et_patriae\"]\n",
    "                                property_data_string_value = random.choice(base_string_list)\n",
    "                                data_node.node_attributes[prop.name] = property_data_string_value\n",
    "            except Exception as e:\n",
    "                print(\"error found\")\n",
    "                print(prop.name)\n",
    "                display_error_line(e)\n",
    "        return\n",
    "    \n",
    "    def get_dict_of_data_nodes(self):\n",
    "        return self.dict_of_data_nodes\n",
    "    \n",
    "    def get_dict_of_data_edges(self):\n",
    "        return self.dict_of_data_edges\n",
    "    \n",
    "    def get_in_degree(self, input_node_id):\n",
    "        for key in self.dict_of_data_nodes:\n",
    "            for node in dict_of_data_nodes[key]:\n",
    "                if node.node_id == input_node_id:\n",
    "                    return len(node.parent_node_id_list)\n",
    "    \n",
    "    def get_out_degree(self, input_node_id):\n",
    "        for key in self.dict_of_data_nodes:\n",
    "            for node in dict_of_data_nodes[key]:\n",
    "                if node.node_id == input_node_id:\n",
    "                    return len(node.child_node_id_list)\n",
    "    \n",
    "    def summary(self):\n",
    "        summary = {}\n",
    "        summary['Nodes Summary'] = {}\n",
    "        summary['Edges Summary']  = {}\n",
    "        \n",
    "        for node_type in self.dict_of_data_nodes:\n",
    "            node_count = len(dict_of_data_nodes[node_type])\n",
    "            summary['Nodes Summary'].update({node_type: node_count})\n",
    "        \n",
    "        edge_type_list = []\n",
    "        for edge in dict_of_data_edges.values():\n",
    "            edge_type_list.append(edge.edge_type)\n",
    "        edge_type_counter = Counter(edge_type_list)\n",
    "        for edge_type, edge_type_count in edge_type_counter.items():\n",
    "            summary['Edges Summary'].update({edge_type: edge_type_count})\n",
    "        \n",
    "        return summary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_error_line(ex):\n",
    "    trace = []\n",
    "    tb = ex.__traceback__\n",
    "    while tb is not None:\n",
    "        trace.append({\"filename\": tb.tb_frame.f_code.co_filename, \"name\": tb.tb_frame.f_code.co_name, \"lineno\": tb.tb_lineno})\n",
    "        tb = tb.tb_next\n",
    "    print(str({'type': type(ex).__name__, 'message': str(ex), 'trace': trace}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN READ SECTION\n"
     ]
    }
   ],
   "source": [
    "######BEGIN READ SECTION######\n",
    "print('BEGIN READ SECTION')\n",
    "dict_of_model_properties = {}\n",
    "dict_of_model_nodes = {}\n",
    "model_graph = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sep = os.path.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_files = './popsci-mock-data-config-files.yaml'\n",
    "with open(configuration_files) as f:\n",
    "    configuration_files = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NODE_FILE': '..\\\\Data_Loader\\\\model_files\\\\popsci-model.yml',\n",
       " 'PROP_FILE': '..\\\\Data_Loader\\\\model_files\\\\popsci-model-props.yml',\n",
       " 'ID_FILE': '..\\\\Data_Loader\\\\model_files\\\\props-popsci.yml',\n",
       " 'LOCATION_CONV_FILE': '..\\\\Data_Loader\\\\model_files\\\\cancer_site_location.yaml',\n",
       " 'HISTOLOGY_CONV_FILE': '..\\\\Data_Loader\\\\model_files\\\\cancer_histology_v2.yaml',\n",
       " 'SYNTHETIC_DATA_FILE': '.\\\\cds-synthetic_data_values.xlsx',\n",
       " 'DATA_SPEC_FILE': '.\\\\popsci-mock-data-specs_v2.yaml',\n",
       " 'STUDY_FILE': '.\\\\Study_Files\\\\',\n",
       " 'OUTPUT_FOLDER': '.\\\\Mock_Data_Output\\\\'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site = data[index][1]\n",
    "#sub_region = data[index][2]\n",
    "#Histology Description = data[index][3]\n",
    "#code = data[index][4]\n",
    "#translation = data[index][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tumor_dict =   {'0': 'Benign', \n",
    "#                '1': 'Uncertain whether benign or malignant',\n",
    "#                '2': 'Carcinoma in situ',\n",
    "#                '3': 'Malignant, primary site',\n",
    "#                '6': 'Malignant, metastatic site',\n",
    "#                '9': 'Malignant, uncertain whether primary or metastatic site'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ MODEL FILES AND FILE WITH SYNTHETIC VALUES\n",
    "#FOR BENTO\n",
    "NODE_FILE = configuration_files['NODE_FILE']\n",
    "PROP_FILE = configuration_files['PROP_FILE']\n",
    "SPEC_FILE = configuration_files['DATA_SPEC_FILE']\n",
    "SYNTHETIC_DATA_FILE = configuration_files['SYNTHETIC_DATA_FILE']\n",
    "\n",
    "cancer_location_conv = configuration_files['LOCATION_CONV_FILE']\n",
    "cancer_histology_conv = configuration_files['HISTOLOGY_CONV_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_values_df = pd.read_excel(io = SYNTHETIC_DATA_FILE,\n",
    "                        sheet_name = \"Sheet1\",\n",
    "                        engine = \"openpyxl\",\n",
    "                        keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer_term_file = r'C:\\Users\\breadsp2\\Desktop\\Current_Model_Files\\cancer_site_location.yaml'\n",
    "with open(cancer_location_conv) as f:\n",
    "    cancer_term_file = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "all_terms = pd.DataFrame()\n",
    "for curr_location in cancer_term_file[\"CancerLocation\"]:\n",
    "    x = pd.DataFrame.from_dict(cancer_term_file[\"CancerLocation\"][curr_location], orient = 'index')\n",
    "    x[\"Primary Site\"] = curr_location\n",
    "    x.reset_index(inplace=True)\n",
    "    \n",
    "    all_terms = pd.concat([all_terms, x])\n",
    "    \n",
    "all_terms.columns = [[\"Sub Site\", \"ICD-0-3 Code\", \"Primary Site\"]]\n",
    "cancer_location_df = all_terms [[ \"Primary Site\", \"Sub Site\", \"ICD-0-3 Code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer_term_file = r'C:\\Users\\breadsp2\\Desktop\\Current_Model_Files\\cancer_site_location.yaml'\n",
    "with open(cancer_histology_conv) as f:\n",
    "    cancer_term_file = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "#all_terms = pd.DataFrame()\n",
    "all_terms = pd.DataFrame.from_dict(cancer_term_file[\"CancerHistology\"], orient = 'index')\n",
    "all_terms.reset_index(inplace=True)    \n",
    "\n",
    "#for curr_location in cancer_term_file[\"CancerHistology\"]:\n",
    "    #x = pd.DataFrame.from_dict(cancer_term_file[\"CancerHistology\"][curr_location], orient = 'index')\n",
    "    #x[\"Primary Site\"] = curr_location\n",
    "    #all_terms = pd.concat([all_terms, x])\n",
    "    \n",
    "all_terms.columns = [\"ICD-0-3 Code\", \"Primary Morphology\"]\n",
    "cancer_histology_df = all_terms[[\"Primary Morphology\", \"ICD-0-3 Code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SPEC_FILE) as f:\n",
    "    data_spec= yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    #IncludeProperties = spec_data['IncludeProperties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_data = data_spec['WeightProperties']    \n",
    "weight_dict = {}\n",
    "for weight_name in weighted_data.keys():\n",
    "    temp_dict = {}\n",
    "    [temp_dict.update(i) for i in weighted_data[weight_name]]\n",
    "    weight_dict.update({weight_name: temp_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_location_type = data_spec[\"ICD_CodePropertes\"][\"location\"]\n",
    "cancer_histology_type = data_spec[\"ICD_CodePropertes\"][\"histology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(WEIGHT_FILE) as f:\n",
    "#    weighted_data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROP_FILE) as f:\n",
    "    property_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for property_name in property_data['PropDefinitions'].keys():\n",
    "        try:\n",
    "            property_value_type = property_data['PropDefinitions'][property_name]['Type']\n",
    "        except:\n",
    "            try:\n",
    "                #property_value_type = property_data['PropDefinitions'][property_name]['Enum']\n",
    "                property_value_type = property_data['PropDefinitions'][property_name]\n",
    "            except:\n",
    "                property_value_type = 'string'\n",
    "                print(property_name)\n",
    "        \n",
    "        name = property_name\n",
    "        desc = \"\"\n",
    "        req= \"\"\n",
    "        value_type = \"\"\n",
    "        value_list = []\n",
    "        synthetic_value_list = []\n",
    "        units = []\n",
    "        private = \"\"\n",
    "        pattern = \"\"\n",
    "        url = \"\"\n",
    "        minimum = 0\n",
    "        maximum = 1000\n",
    "        exclusiveMinimum = \"\"\n",
    "        exclusiveMaximum = \"\"\n",
    "        #dist = 'Uniform'\n",
    "        #weights = []\n",
    "\n",
    "        if type(property_value_type) is str:\n",
    "            name = property_name\n",
    "            value_type = property_value_type\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            if 'minimum' in property_data['PropDefinitions'][property_name]:\n",
    "                minimum = property_data['PropDefinitions'][property_name]['minimum']\n",
    "            if 'maximum' in property_data['PropDefinitions'][property_name]:\n",
    "                maximum = property_data['PropDefinitions'][property_name]['maximum']\n",
    "\n",
    "                \n",
    "        if type(property_value_type) is list:\n",
    "            value_type = \"list\"\n",
    "            value_list = property_value_type['Enum']\n",
    "            #add section on reading the url to create a value list if property_value_type contains a url.\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            #if 'dist' in property_data['PropDefinitions'][property_name]:\n",
    "            #    dist = property_data['PropDefinitions'][property_name]['dist']\n",
    "            #if 'weights' in property_data['PropDefinitions'][property_name]:\n",
    "            #    weights = property_data['PropDefinitions'][property_name]['weights']\n",
    "\n",
    "                \n",
    "        if type(property_value_type) is dict:\n",
    "            if 'Desc' in property_data['PropDefinitions'][property_name]:\n",
    "                desc = property_data['PropDefinitions'][property_name]['Desc']\n",
    "            if 'value_type' in property_value_type:\n",
    "                value_type = property_value_type['value_type']\n",
    "            if \"Enum\" in property_value_type:\n",
    "                value_list = property_value_type[\"Enum\"]\n",
    "            if 'units' in property_value_type:\n",
    "                units = property_value_type['units']\n",
    "            if 'pattern' in property_value_type:\n",
    "                pattern = property_value_type['pattern']\n",
    "                value_type = \"regex\"\n",
    "            if 'Req' in property_data['PropDefinitions'][property_name]:\n",
    "                req = property_data['PropDefinitions'][property_name]['Req']\n",
    "            if 'Private' in property_data['PropDefinitions'][property_name]:\n",
    "                private = property_data['PropDefinitions'][property_name]['Private']\n",
    "            if 'minimum' in property_data['PropDefinitions'][property_name]:\n",
    "                minimum = property_data['PropDefinitions'][property_name]['minimum']\n",
    "            if 'maximum' in property_data['PropDefinitions'][property_name]:\n",
    "                maximum = property_data['PropDefinitions'][property_name]['maximum']\n",
    "            \n",
    "        dict_of_model_properties[property_name] = ModelProperty(name = name, desc = desc, \n",
    "                                                                    value_type = value_type, value_list = value_list,\n",
    "                                                                    units = units, url = url, req = req, private = private, minimum = minimum, maximum = maximum,\n",
    "                                                                    exclusiveMinimum = exclusiveMinimum, exclusiveMaximum = exclusiveMaximum, synthetic_value_list = synthetic_value_list)\n",
    "                                                                    # ,dist = dist, weights = weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODE_FILE) as f:\n",
    "    node_data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property: data_file_access_control has issues for node: study\n",
      "property: data_file_access_control has issues for node: study_personnel\n"
     ]
    }
   ],
   "source": [
    "nodes = node_data['Nodes']\n",
    "\n",
    "for node_name in nodes.keys():\n",
    "    #print(node_name, nodes[node_name]['Props'])\n",
    "    if nodes[node_name]['Props']:\n",
    "        try:\n",
    "            property_list = [dict_of_model_properties[property_name] for property_name in nodes[node_name]['Props']]\n",
    "        except Exception:\n",
    "            print(f\"property: {property_name} has issues for node: {node_name}\")\n",
    "            property_list = []\n",
    "    else:\n",
    "        property_list = []\n",
    "    dict_of_model_nodes[node_name] = ModelNode(name = node_name, properties = property_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END READ SECTION\n"
     ]
    }
   ],
   "source": [
    "edges = node_data['Relationships']\n",
    "\n",
    "for edge_name in edges.keys():\n",
    "    Ends_list = []\n",
    "    Property_list = []\n",
    "    edge_multiplicity = edges[edge_name]['Mul']\n",
    "    \n",
    "    for edge_pair in edges[edge_name]['Ends']:\n",
    "        source_node = edge_pair['Src']\n",
    "        destination_node = edge_pair['Dst']\n",
    "        if 'Mul' in edge_pair:\n",
    "            edge_multiplicity = edge_pair['Mul']\n",
    "        Ends_list.append(ModelEnds(source_node = dict_of_model_nodes[source_node], destination_node = dict_of_model_nodes[destination_node], multiplicity = edge_multiplicity))\n",
    "        \n",
    "    \n",
    "    if 'Props' in edges[edge_name] and edges[edge_name]['Props']:\n",
    "        property_list = [dict_of_model_properties[property_name] for property_name in edges[edge_name]['Props']]\n",
    "    else:\n",
    "        property_list = []\n",
    "    \n",
    "    model_graph[edge_name] = ModelEdge(name = edge_name, ends_list = Ends_list, properties_list = Property_list)\n",
    "#END READ MODEL FILES\n",
    "print('END READ SECTION')\n",
    "######END READ SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN SPAWN SECTION\n"
     ]
    }
   ],
   "source": [
    "######BEGIN SPAWN SECTION######\n",
    "print('BEGIN SPAWN SECTION')\n",
    "dict_of_data_nodes = defaultdict(list)\n",
    "dict_of_data_edges = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ DATA SPECS FILE\n",
    "#FOR BENTO\n",
    "DATA_SPEC_FILE = configuration_files['DATA_SPEC_FILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_SPEC_FILE) as f:\n",
    "    data_spec = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_FILE = configuration_files['ID_FILE']\n",
    "with open(ID_FILE) as f:\n",
    "    id_field_data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICDO_CONV_FILE = configuration_files['ICDO_CONV_FILE']\n",
    "#with open(ICDO_CONV_FILE) as f:\n",
    "#    id_conv_file = pd.read_excel(ICDO_CONV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_study_file(study_path):\n",
    "    data = []\n",
    "    with open(study_path, 'r') as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in tsvreader:\n",
    "            data.append(row)\n",
    "    study_df = pd.DataFrame(columns = data[0])\n",
    "    study_df.loc[len(study_df)] = data[1]\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "includeNodes = data_spec['IncludeNodes']\n",
    "Node_Props = data_spec['MockDataProps']\n",
    "Output_Folder = configuration_files['OUTPUT_FOLDER'] +  data_spec['MockDataProps'][\"StudyName\"] + \"_Mock_Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create head data node object\n",
    "for head_node in data_spec['HeadNode']:\n",
    "    head_node_type = head_node['name']\n",
    "    if Node_Props['ImportStudyFile']:\n",
    "        IMPORT_STUDY = True\n",
    "        study_name = configuration_files['STUDY_FILE'] + data_spec['MockDataProps'][\"StudyName\"] +  \"-study.txt\"\n",
    "        study_df = import_study_file(study_name)\n",
    "    else:\n",
    "        IMPORT_STUDY = False\n",
    "        study_name = 'Random_Study'\n",
    "    if head_node_type in data_spec['IncludeNodes'].keys():\n",
    "        logging.error('HeadNode ' + head_node_type + 'is in the IncludeNodes.')\n",
    "        sys.exit()\n",
    "    head_node_count = head_node['count']\n",
    "    id_prefix = head_node['Prefix']\n",
    "    dst_node_type = head_node_type\n",
    "    # random a set of id without duplicate\n",
    "    node_id_number_list = random.sample(range(10**5, 10**7), head_node_count + 1)\n",
    "    head_node_index = 0\n",
    "    for count in range(head_node_count):\n",
    "        # node_id = id_prefix + \"_\" + str(random.randint(10**5, 10**6))\n",
    "        \n",
    "        if IMPORT_STUDY == True and head_node_type  == \"study\":\n",
    "            field_name = id_field_data['Properties']['id_fields'][head_node_type]\n",
    "            node_id = study_df.iloc[0][field_name]\n",
    "        else: \n",
    "            node_id =  id_prefix + \"-\" + str(node_id_number_list[head_node_index]).zfill(7)\n",
    "            #node_id = id_prefix + \"-\" + str(node_id_number_list[head_node_index])# for bento\n",
    "        \n",
    "        head_node_index += 1\n",
    "        parent_node_id_list = []\n",
    "        child_node_id_list = []\n",
    "        node_type = head_node_type\n",
    "        node_attributes = {}\n",
    "        data_node = DataNode(node_id = node_id, parent_node_id_list = parent_node_id_list, child_node_id_list = child_node_id_list,\n",
    "                             node_type = node_type, node_attributes = {})\n",
    "        dict_of_data_nodes[head_node_type].append(data_node)\n",
    "\n",
    "edge_specs = data_spec['RelationshipSpecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dst_node_type in edge_specs.keys():\n",
    "    dst_data_nodes_list = dict_of_data_nodes[dst_node_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEdgeType(node_data, src_node_type, dst_node_type):\n",
    "    for edge_type in node_data['Relationships']:\n",
    "        for ends in node_data['Relationships'][edge_type]['Ends']:\n",
    "            if ends['Src'] == src_node_type and ends['Dst'] == dst_node_type:\n",
    "                return edge_type\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a skeleton data graph.\n",
    "#Create a skeleton data graph.\n",
    "def SpawnNodes():\n",
    "    created_children = []\n",
    "    children = []\n",
    "    for dst_node_type in edge_specs.keys():\n",
    "        print(f\"working on { dst_node_type}\")\n",
    "        if dst_node_type in dict_of_data_nodes and dst_node_type not in created_children:\n",
    "            dst_data_nodes_list = dict_of_data_nodes[dst_node_type]\n",
    "            # for dst_data_node in dst_data_nodes_list:\n",
    "            for src_node_type in edge_specs[dst_node_type].keys():\n",
    "                print(f\"check releationship: {dst_node_type}, {src_node_type}\")\n",
    "                node_counter = includeNodes[src_node_type]['NodeCount']\n",
    "                node_distribution = edge_specs[dst_node_type][src_node_type]['SrcNodeCount']\n",
    "                id_prefix = includeNodes[src_node_type]['Prefix']\n",
    "                # random a set of id without duplicate\n",
    "                node_id_number_list = random.sample(range(10**5, 10**7), node_counter)\n",
    "                node_index = 0\n",
    "                parent_node_index = 0\n",
    "                parent_node_length = len(dst_data_nodes_list)\n",
    "                if parent_node_length == 0:\n",
    "                    continue\n",
    "                \n",
    "                step = int(node_counter / parent_node_length)\n",
    "                # if the distribution is random\n",
    "                # print(node_counter, parent_node_length)\n",
    "                if node_distribution == 'random':\n",
    "                    node_counter_list = range(node_counter)\n",
    "                    random_split_points = random.sample(node_counter_list, parent_node_length - 1)\n",
    "                    random_split_points.sort()\n",
    "                    random_split_points_index = 0\n",
    "                for count in range(node_counter):\n",
    "                    if node_distribution == 'fixed':\n",
    "                        if node_index % step == 0 and node_index != 0:\n",
    "                            parent_node_index += 1\n",
    "                    elif node_distribution == 'random':\n",
    "                        #print(node_index)\n",
    "                        if random_split_points_index < len(random_split_points):\n",
    "                            if node_index == random_split_points[random_split_points_index]:\n",
    "                                random_split_points_index += 1\n",
    "                                parent_node_index += 1\n",
    "                    if parent_node_index > parent_node_length - 1:\n",
    "                            parent_node_index = parent_node_length - 1\n",
    "                    node_id = data_spec['MockDataProps'][\"StudyName\"] + \"-\" + id_prefix + \"-\" + str(node_id_number_list[node_index]).zfill(7)\n",
    "                    if src_node_type not in children:\n",
    "                        node_index += 1\n",
    "                        parent_node_id_list = []\n",
    "                        parent_node_id_list.append(dst_data_nodes_list[parent_node_index].node_id)\n",
    "                        child_node_id_list = []\n",
    "                        node_type = src_node_type\n",
    "                        node_attributes = {}\n",
    "                        src_data_node = DataNode(node_id = node_id, parent_node_id_list = parent_node_id_list, child_node_id_list = child_node_id_list,\n",
    "                                             node_type = node_type, node_attributes = {}) #source node created.\n",
    "                        dict_of_data_nodes[src_node_type].append(src_data_node) #source node added to the dict of nodes.\n",
    "\n",
    "                        dst_data_nodes_list[parent_node_index].child_node_id_list.append(node_id) #add created source node to the child nodes list for dst node.\n",
    "                    elif src_node_type in children:\n",
    "                        dict_of_data_nodes[src_node_type][node_index].parent_node_id_list.append(dst_data_nodes_list[parent_node_index].node_id)\n",
    "                        node_index += 1\n",
    "                    edge_id = \"edge\" + \"_\" + str(random.randint(10**5, 10**6))\n",
    "                    edge_type = findEdgeType(node_data, src_node_type, dst_node_type)\n",
    "                    # edge_type = edge_specs[dst_node_type][src_node_type]['EdgeType']\n",
    "                    edge_attributes = {}\n",
    "                    data_edge = DataEdge(edge_id = edge_id, edge_type = edge_type, source_node = src_data_node, \n",
    "                                     destination_node = dst_data_nodes_list[parent_node_index], edge_attributes = edge_attributes) #edge created.\n",
    "                    dict_of_data_edges[edge_id] = data_edge #edge added to the dict of edges.\n",
    "                children.append(src_node_type)\n",
    "            created_children.append(dst_node_type)\n",
    "    data_graph = Graph(dict_of_data_nodes = dict_of_data_nodes, dict_of_data_edges = dict_of_data_edges)\n",
    "    return data_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on study\n",
      "check releationship: study, participant\n"
     ]
    }
   ],
   "source": [
    "#Create skeleton data graph\n",
    "data_graph = SpawnNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Edges Summary': {'associated_with': 19781},\n",
      " 'Nodes Summary': {'participant': 20000, 'study': 1}}\n",
      "END SPAWN SECTION\n"
     ]
    }
   ],
   "source": [
    "#Examine skeleton data graph\n",
    "# data_graph.summary()\n",
    "summary = data_graph.summary()\n",
    "pprint.pprint(summary)\n",
    "print('END SPAWN SECTION')\n",
    "######END SPAWN SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_node_dict = {}\n",
    "node_id_field_dict = {}\n",
    "node_list = []\n",
    "for parent_id in data_spec['RelationshipSpecs']:\n",
    "    for node_id in data_spec['RelationshipSpecs'][parent_id]:\n",
    "        relationship_node = {}\n",
    "        relationship_node['parent_id'] = []\n",
    "        relationship_node['parent_id_field'] = []\n",
    "        relationship_node['node_id'] = node_id\n",
    "        relationship_node['parent_id'].append(parent_id)\n",
    "        if node_id in id_field_data['Properties']['id_fields']:\n",
    "            relationship_node['node_id_field'] = id_field_data['Properties']['id_fields'][node_id]\n",
    "        else:\n",
    "            relationship_node['node_id_field'] ='node_id'\n",
    "        if parent_id in id_field_data['Properties']['id_fields']:\n",
    "            relationship_node['parent_id_field'].append(id_field_data['Properties']['id_fields'][parent_id])\n",
    "        else:\n",
    "            relationship_node['parent_id_field'].append('parent_id')\n",
    "        if node_id not in relationship_node_dict.keys():\n",
    "            relationship_node_dict[node_id] = relationship_node\n",
    "        else:\n",
    "            relationship_node_dict[node_id]['parent_id'].append(relationship_node['parent_id'][0])\n",
    "            relationship_node_dict[node_id]['parent_id_field'].append(relationship_node['parent_id_field'][0])\n",
    "\n",
    "for node_type in data_graph.dict_of_data_nodes:\n",
    "    if node_type in id_field_data['Properties']['id_fields']:\n",
    "        node_id_field_dict[node_type] = id_field_data['Properties']['id_fields'][node_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetParentIDField(node_type, index):\n",
    "    if node_type in relationship_node_dict:\n",
    "        if isinstance(relationship_node_dict[node_type]['parent_id_field'][index], list):\n",
    "            return relationship_node_dict[node_type]['parent_id'][index]+'.'+relationship_node_dict[node_type]['parent_id_field'][index][0]\n",
    "        else:\n",
    "            return relationship_node_dict[node_type]['parent_id'][index]+'.'+relationship_node_dict[node_type]['parent_id_field'][index]\n",
    "    else:\n",
    "        return 'parent_id'\n",
    "def GetNodeIDField(node_type):\n",
    "    if node_type in node_id_field_dict:\n",
    "        if isinstance(node_id_field_dict[node_type], list):\n",
    "            return node_id_field_dict[node_type][0]\n",
    "        else:\n",
    "            return node_id_field_dict[node_type]\n",
    "    else:\n",
    "        return 'node_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN FILL SECTION\n"
     ]
    }
   ],
   "source": [
    "######BEGIN FILL SECTION######\n",
    "print('BEGIN FILL SECTION')\n",
    "includePropsList = data_spec['IncludeProperties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating node_type for study\n",
      "Populating node_type for participant\n",
      "END FILL SECTION\n"
     ]
    }
   ],
   "source": [
    "data_graph.fill_graph(listOfProps = includePropsList, \n",
    "                      model_nodes_dict = dict_of_model_nodes, \n",
    "                      model_props_dict = dict_of_model_properties)         \n",
    "\n",
    "print('END FILL SECTION')\n",
    "######END FILL SECTION######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINT DATA FILES\n",
      "working on study file\n",
      "working on participant file\n",
      "Writting file: .\\Mock_Data_Output\\Study_PLTV\\participant.tsv\n"
     ]
    }
   ],
   "source": [
    "######PRINT DATA FILES######\n",
    "print('PRINT DATA FILES')\n",
    "child_node_id_dict = {}\n",
    "child_node_id_list = []\n",
    "for node_type in data_graph.dict_of_data_nodes:\n",
    "    node_values_dict = defaultdict(list)\n",
    "    df = pd.DataFrame()\n",
    "    position = 0\n",
    "    for node in data_graph.dict_of_data_nodes[node_type]:\n",
    "\n",
    "        node_values_dict['type'].append(node.node_type)\n",
    "        if node.parent_node_id_list:\n",
    "            index = 0\n",
    "            for parent_node_id in node.parent_node_id_list:\n",
    "                if node.node_id not in child_node_id_list:\n",
    "                    node_values_dict[GetParentIDField(node_type, index)].append(parent_node_id) #parent\n",
    "                else:\n",
    "                    node_values_dict[GetParentIDField(node_type, index)].append(child_node_id_dict[node.node_id])\n",
    "                    data_graph.dict_of_data_nodes[node_type][position].parent_node_id_list[0] = child_node_id_dict[node.node_id]\n",
    "                index += 1\n",
    "        \n",
    "        if GetNodeIDField(node_type) not in node.node_attributes:\n",
    "            node_values_dict[GetNodeIDField(node_type)].append(node.node_id) #node\n",
    "\n",
    "\n",
    "        if GetNodeIDField(node_type) in node.node_attributes and GetNodeIDField(node_type) in synthetic_values_df.keys():\n",
    "            res = synthetic_values_df[GetNodeIDField(node_type)].tolist()\n",
    "            # res = synthetic_values_df[GetNodeIDField(node_type)].tolist()\n",
    "            trim_res = [i for i in res if i]\n",
    "            # if the node_type has more nodes than the values of all usable node_id\n",
    "            if len(data_graph.dict_of_data_nodes[node_type]) > len(trim_res):\n",
    "                error_message = 'node ' + node_type + ' is running out of all usable node_ids from ' + GetNodeIDField(node_type) + '.'\n",
    "                # delete all previous generate tsv files\n",
    "                mydir = os.path.abspath(Output_Folder)\n",
    "                filelist = [ f for f in os.listdir(mydir) if f.endswith(\".tsv\") ]\n",
    "                for f in filelist:\n",
    "                    os.remove(os.path.join(mydir, f))\n",
    "                sys.exit(error_message)\n",
    "            new_node_id_list = []\n",
    "            for new_node in data_graph.dict_of_data_nodes[node_type]:\n",
    "                new_node_id_list.append(new_node.node_attributes[GetNodeIDField(node_type)])\n",
    "            new_node_id_list_counter = Counter(new_node_id_list)\n",
    "            reselect_value = False\n",
    "            #Check if the new node_id list has duplicate node_id\n",
    "            for value in new_node_id_list_counter.values():\n",
    "                if value > 1:\n",
    "                    reselect_value = True\n",
    "            if reselect_value == True:\n",
    "                new_value_list = random.sample(trim_res, len(new_node_id_list))\n",
    "                for i in range(len(data_graph.dict_of_data_nodes[node_type])):\n",
    "                    data_graph.dict_of_data_nodes[node_type][i].node_attributes[GetNodeIDField(node_type)] = new_value_list[i]\n",
    "            data_graph.dict_of_data_nodes[node_type][position].node_id = data_graph.dict_of_data_nodes[node_type][position].node_attributes[GetNodeIDField(node_type)]\n",
    "            for child_node_id in data_graph.dict_of_data_nodes[node_type][position].child_node_id_list:\n",
    "                child_node_id_list.append(child_node_id)\n",
    "                child_node_id_dict[child_node_id] = data_graph.dict_of_data_nodes[node_type][position].node_id\n",
    "        if GetNodeIDField(node_type) in node.node_attributes and GetNodeIDField(node_type) not in synthetic_values_df.keys():\n",
    "            # if the user adds the id field into the data spec document accidentally\n",
    "            del node.node_attributes[GetNodeIDField(node_type)]\n",
    "            node_values_dict[GetNodeIDField(node_type)].append(node.node_id) #node\n",
    "        for node_prop in node.node_attributes:\n",
    "            # print(node_type)\n",
    "            # print(node.node_attributes)\n",
    "            node_values_dict[node_prop].append(node.node_attributes[node_prop])\n",
    "        position+=1\n",
    "    print(f\"working on {node_type} file\")\n",
    "    for node_values_key in node_values_dict:\n",
    "        try:\n",
    "            df[node_values_key] = node_values_dict[node_values_key]\n",
    "        except Exception as e:\n",
    "            print(node_values_key)\n",
    "            \n",
    "    if node_type == 'participant':\n",
    "        control_part = df.query(\"participant_case_indicator == 'No'\")\n",
    "        df.loc[control_part.index,\"cancer_diagnosis_primary_site\"] = 'Not Applicable'\n",
    "        df.loc[control_part.index,\"cancer_diagnosis_disease_morphology\"] = 'Not Applicable'\n",
    "        df.loc[control_part.index,\"age_at_first_cancer_diagnosis\"] = -1\n",
    "        \n",
    "    \n",
    "    #if \"cancer_diagnosis_primary_site\" in df.columns:\n",
    "    #    cancer_location_df = all_terms [[\"cancer_diagnosis_primary_site\", \"cancer_diagnosis_subsite\", \"cancer_diagnosis_ICD-O_location\"]]\n",
    "    #    df = df.merge(cancer_location_df, how=\"left\", left_on=\"cancer_diagnosis_primary_site\", \n",
    "    #                  right_on=\"cancer_diagnosis_ICD-O_location\")\n",
    "        \n",
    "        \n",
    "    #file_index = 1\n",
    "    #while len(df) > 0:\n",
    "    #    if len(df) > 10000:\n",
    "    #        temp_df = df[:10000]\n",
    "    #    else:\n",
    "    #        temp_df = df\n",
    "    \n",
    "    #file_name = configuration_files['OUTPUT_FOLDER'] + node_type + \"_\" + str(file_index) + \".tsv\"\n",
    "    file_name = Output_Folder + file_sep + node_type + \".tsv\"\n",
    "    if not os.path.exists(Output_Folder):\n",
    "        os.mkdir(Output_Folder)\n",
    "    if IMPORT_STUDY == True and node_type == 'study':\n",
    "        file_name =  Output_Folder + file_sep + data_spec['MockDataProps'][\"StudyName\"] +  \"-study.txt\"\n",
    "        study_df = import_study_file(study_name)\n",
    "        study_df.to_csv(file_name, sep = \"\\t\", index = False, na_rep='NULL')\n",
    "    else:\n",
    "        print(f\"Writting file: {file_name}\")\n",
    "        df.to_csv(file_name, sep = \"\\t\", index = False, na_rep='NULL')\n",
    "        #temp_df.to_csv(file_name, sep = \"\\t\", index = False)\n",
    "    #file_index = file_index + 1\n",
    "    #df = df[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#import os\n",
    "#sys.path.append(os.path.abspath(r'C:\\Program Files\\Spyder\\Python\\Lib\\site-packages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "######VALIDATE DATA FILES######\n",
    "#print('VALIDATE DATA FILES')\n",
    "#from data_loader import DataLoader\n",
    "#from icdc_schema import ICDC_Schema\n",
    "#from neo4j import GraphDatabase\n",
    "#from props import Props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_list = [f for f in os.listdir(configuration_files['OUTPUT_FOLDER']) if f.endswith('.tsv')]\n",
    "#for i in range(0, len(file_list)):\n",
    "#    file_list[i] = configuration_files['OUTPUT_FOLDER'] + file_list[i]\n",
    "#props = Props(configuration_files['ID_FILE'])\n",
    "#schema = ICDC_Schema([configuration_files['NODE_FILE'], configuration_files['PROP_FILE']], props)\n",
    "#loader = DataLoader(None, schema)\n",
    "#fileValidationResult = loader.validate_files(False, file_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationshipValidation(dict_of_data_edges, node_data, includeNodes):\n",
    "    for edge in dict_of_data_edges.values():\n",
    "        # print(edge)\n",
    "        mul = node_data['Relationships'][edge.edge_type]['Mul']\n",
    "        prefix = includeNodes[edge.source_node.node_type]['Prefix']\n",
    "        child_node_id_list = []\n",
    "        for child_node_id in edge.destination_node.child_node_id_list:\n",
    "            for prefix in child_node_id:\n",
    "                child_node_id_list.append(child_node_id)\n",
    "        if mul == 'one_to_one' and len(child_node_id_list) > 1:\n",
    "            logging.error(edge.source_node.node_type + ' ' + 'one_to_one relationship failed, parent already has a child!')\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationshipValidationResult = relationshipValidation(dict_of_data_edges, node_data, includeNodes)\n",
    "#if not relationshipValidationResult or not fileValidationResult:\n",
    "#    print('Validation fail, delete all files inside the data folder.')\n",
    "#    mydir = os.path.abspath(configuration_files['OUTPUT_FOLDER'])\n",
    "#    filelist = [ f for f in os.listdir(mydir) if f.endswith(\".tsv\") ]\n",
    "#    for f in filelist:\n",
    "#        os.remove(os.path.join(mydir, f))\n",
    "#else:\n",
    "#    print('Validation success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
